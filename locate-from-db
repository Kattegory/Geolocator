#! /usr/bin/python3

# usage: locate-from-db output-dir calibration basemap database \
#                       [batch selector...]

import argparse
import collections
import contextlib
import csv
import datetime
import gzip
import multiprocessing
import os
import pickle
import sys
import time
import zlib

import psycopg2
import psycopg2.extras

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'lib')))
import ageo

_time_0 = time.monotonic()

def progress(message, *args):
    global _time_0
    sys.stderr.write(
        ("{}: " + message + "\n").format(
            datetime.timedelta(seconds = time.monotonic() - _time_0),
            *args))

def warning(message, *args):
    sys.stderr.write(
        ("\t*** " + message + "\n").format(*args))

def load_calibration(cfname):
    try:
        with gzip.open(cfname, "rb") as fp:
            return pickle.load(fp)

    except (OSError, zlib.error, pickle.UnpicklingError) as e:
        sys.stderr.write("unable to load calibration: {}: {}\n"
                         .format(cfname, e))
        sys.exit(1)

def get_batch_list(db, selector):
    cur = db.cursor()
    query = ("SELECT b.id, COUNT(*) FROM batches b, measurements m "
             "WHERE b.id = m.batch AND m.rtt > 0")
    if selector:
        query += "AND (" + selector + ") "
    query += "GROUP BY b.id;"
    cur.execute(query)
    batches = []
    for row in cur:
        if row[1] > 0:
            batches.append(row[0])

    progress("{} non-empty batches selected.", len(batches))
    return batches

Position = collections.namedtuple("Position", ("lon", "lat"))
def get_landmark_positions(db, batches):
    cur = db.cursor()
    cur.execute("SELECT DISTINCT h.ipv4, h.longitude, h.latitude"
                "  FROM hosts h, measurements m"
                " WHERE m.dst = h.ipv4"
                "   AND m.batch = ANY(%s)",
                (batches,))
    return { row[0]: Position(row[1], row[2])
             for row in cur }

def retrieve_batch(db, batchid):
    cur = db.cursor(cursor_factory=psycopg2.extras.DictCursor)
    cur.execute("""
        SELECT b.id, b.client_lat, b.client_lon, b.client_addr,
               c.label, c.country, c.asn,
               b.proxied, b.proxy_lat, b.proxy_lon, b.proxy_addr,
               p.label, p.country, p.asn
          FROM batches b
     LEFT JOIN hosts c ON b.client_addr = c.ipv4
     LEFT JOIN hosts p ON b.proxy_addr  = p.ipv4
         WHERE b.id = %s""", (batchid,))

    # Copy the metadata into a normal dictionary to avoid problems later
    # when it gets stuffed into a Location annotation.
    metadata_raw = cur.fetchone()
    metadata = {}
    metadata.update(metadata_raw)

    # We don't need a fancy cursor for the next step.
    cur = db.cursor()
    # There's only ever one source for any given batch (and it is always
    # equal to either client_addr or proxy_addr for that batch).
    # Throw out all measurements that didn't come back with either
    # errno 0 or 111 (that is, success and ECONNREFUSED).  Also throw
    # out RTT zero, which tends to make the calibration choke.
    cur.execute("SELECT dst, rtt FROM measurements"
                " WHERE batch = %s AND rtt > 0 AND status IN (0, 111)"
                " ORDER BY rtt, dst", (batchid,))

    measurements = collections.defaultdict(list)
    if metadata['proxied']:
        adjustment = None
    else:
        adjustment = 0
    for dst, rtt in cur:
        if not (0 <= rtt < 5000):
            warning("out of range: {} -> {}: {}", batchid, dst, rtt)
            continue

        if adjustment is None:
            # For measurements through a proxy, we subtract off 5 ms
            # less than the smallest observed RTT measurement; this is
            # a cheap and good-enough-we-hope way to cut out the
            # travel time _to_ the proxy, without risking other
            # problems.  (The measurement procedure attempted to
            # identify and ping the proxy's router; if that worked, it
            # should be responsible for the lowest observed pings.)
            adjustment = max(rtt - 5, 0)

        measurements[dst].append(rtt - adjustment)


    return metadata, measurements

# these are filled in in main()
positions    = None
basemap      = None
calibrations = None

def process_batch(args):
    global positions, calibrations, basemap
    odir, mode, metadata, measurements = args
    tag, cals, ranging, use_all = calibrations[mode]
    bnd = basemap.bounds
    obsv = []
    for landmark, rtts in measurements.items():
        if landmark not in positions: continue
        if landmark not in cals: continue
        lpos = positions[landmark]

        obs = ageo.Observation(
            basemap=basemap,
            ref_lat=lpos.lat,
            ref_lon=lpos.lon,
            range_fn=ranging,
            calibration=cals[0] if use_all else cals[landmark],
            rtts=rtts)
        obsv.append(obs)
        bnd = bnd.intersection(obs.bounds)
        if bnd.is_empty:
            return tag, str(metadata['id']) + " (did not converge)"

    loc = obsv[0]
    for obs in obsv[1:]:
        loc = loc.intersection(obs, bnd)

    loc.annotations.update(metadata)
    loc.save(os.path.join(odir, tag + "-" + str(metadata['id']) + ".h5"))
    return tag, metadata['id']

def marshal_batches(args, db, batches, modes):
    for batchid in batches:
        metadata, measurements = retrieve_batch(db, batchid)
        for mode in modes:
            yield (args.output_dir, mode, metadata, measurements)

def inner_main(args, pool, db, batches):
    global calibrations

    modes = range(len(calibrations))
    for tag, id in pool.imap_unordered(
            process_batch,
            marshal_batches(args, db, batches, modes)):
        progress("{}: {}", id, tag)

def main():
    global positions, calibrations, basemap

    ap = argparse.ArgumentParser()
    ap.add_argument("output_dir")
    ap.add_argument("calibration")
    ap.add_argument("basemap")
    ap.add_argument("database")
    ap.add_argument("batch_selector", nargs=argparse.REMAINDER)
    args = ap.parse_args()

    args.batch_selector = " ".join(args.batch_selector)

    # The worker pool must be created before the database connection,
    # so that the database handle is not duplicated into the worker
    # processes.  However, we also need the database connection before
    # the worker processes exist, to load up 'positions', which is
    # propagated into the worker processes by fork().  So we have to
    # drop the database connection and pick it back up again.
    progress("preparing...")
    os.makedirs(args.output_dir, exist_ok=True)
    calibrations = load_calibration(args.calibration)
    basemap = ageo.Map(args.basemap)
    with contextlib.closing(psycopg2.connect(dbname=args.database)) as db:
        batches = get_batch_list(db, args.batch_selector)
        positions = get_landmark_positions(db, batches)

    with multiprocessing.Pool() as pool:
        with contextlib.closing(psycopg2.connect(dbname=args.database)) as db:
            inner_main(args, pool, db, batches)

main()
